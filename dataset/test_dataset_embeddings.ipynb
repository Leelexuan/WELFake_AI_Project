{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e27bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f923a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "# Define device for torch\n",
    "use_cuda = True\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0a4a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 6900/6900 [00:00<00:00, 38734.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'tfidf_embedding', 'bow_embedding', 'w2v_embeddings'],\n",
      "        num_rows: 6900\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset(\"lelexuanzz/Gossipcop_Politifact_Test\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cab877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf_embedding</th>\n",
       "      <th>bow_embedding</th>\n",
       "      <th>w2v_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The media reported on “5 TikTok dances you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32230591024079835, -0.0283119498671771, 0.5...</td>\n",
       "      <td>[4.111553694916509, -1.409115414997175, -2.222...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Over 240,000 'unverified' ballots have alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3085783060828312, 0.11941719274360035, 0.04...</td>\n",
       "      <td>[15.340171294779946, 0.6782201912562608, -3.95...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says \"Ron Johnson is making excuses for rioter...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4233043848185321, 0.07862740948902779, 0.05...</td>\n",
       "      <td>[17.57951108926487, -3.370470538016004, -6.032...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We have seen over the last 10 years ... under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3235193218432442, 0.14350347893253146, -0.0...</td>\n",
       "      <td>[13.623137823124136, 2.594215458879296, 1.1975...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t get involved in the hiring and firing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21714023268740257, -0.01335753130769729, 0....</td>\n",
       "      <td>[11.715673205868844, -3.897763995131928, -0.56...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>Wedding Album: Dancing with the Stars Pro Lind...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14163919484466825, -0.18715353304800564, -0...</td>\n",
       "      <td>[0.5081918596591114, -0.4770132598956357, 0.04...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>WATCH: Sneak Peek: Arizona's Furious Alex Atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10089287837511274, -0.1537017105814096, -0....</td>\n",
       "      <td>[0.3445347175467952, -0.49345817219713883, 0.0...</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Mary Kay Letourneau 'Hopeful' She Can Fix Marr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.30012685395643773, -0.26290611124497976, -0...</td>\n",
       "      <td>[5.587042723074906, -3.0189127373273474, -0.69...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Charlize Theron still upset Aeon Flux didn’t w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18598375894519942, -0.26601918527230306, -0...</td>\n",
       "      <td>[1.8348212505816772, -2.5021137033708087, 0.45...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Debbie Gibson Says ‘DWTS’ Was a Healing Experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14173209453102945, -0.16330857496865459, -0...</td>\n",
       "      <td>[1.0371355773081117, -0.8878316745599937, -0.0...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6900 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     The media reported on “5 TikTok dances you can...      0   \n",
       "1     \"Over 240,000 'unverified' ballots have alread...      0   \n",
       "2     Says \"Ron Johnson is making excuses for rioter...      1   \n",
       "3     “We have seen over the last 10 years ... under...      0   \n",
       "4     “I don’t get involved in the hiring and firing...      0   \n",
       "...                                                 ...    ...   \n",
       "6895  Wedding Album: Dancing with the Stars Pro Lind...      0   \n",
       "6896  WATCH: Sneak Peek: Arizona's Furious Alex Atta...      0   \n",
       "6897  Mary Kay Letourneau 'Hopeful' She Can Fix Marr...      0   \n",
       "6898  Charlize Theron still upset Aeon Flux didn’t w...      0   \n",
       "6899  Debbie Gibson Says ‘DWTS’ Was a Healing Experi...      0   \n",
       "\n",
       "                                        tfidf_embedding  \\\n",
       "0     [0.32230591024079835, -0.0283119498671771, 0.5...   \n",
       "1     [0.3085783060828312, 0.11941719274360035, 0.04...   \n",
       "2     [0.4233043848185321, 0.07862740948902779, 0.05...   \n",
       "3     [0.3235193218432442, 0.14350347893253146, -0.0...   \n",
       "4     [0.21714023268740257, -0.01335753130769729, 0....   \n",
       "...                                                 ...   \n",
       "6895  [0.14163919484466825, -0.18715353304800564, -0...   \n",
       "6896  [0.10089287837511274, -0.1537017105814096, -0....   \n",
       "6897  [0.30012685395643773, -0.26290611124497976, -0...   \n",
       "6898  [0.18598375894519942, -0.26601918527230306, -0...   \n",
       "6899  [0.14173209453102945, -0.16330857496865459, -0...   \n",
       "\n",
       "                                          bow_embedding  \\\n",
       "0     [4.111553694916509, -1.409115414997175, -2.222...   \n",
       "1     [15.340171294779946, 0.6782201912562608, -3.95...   \n",
       "2     [17.57951108926487, -3.370470538016004, -6.032...   \n",
       "3     [13.623137823124136, 2.594215458879296, 1.1975...   \n",
       "4     [11.715673205868844, -3.897763995131928, -0.56...   \n",
       "...                                                 ...   \n",
       "6895  [0.5081918596591114, -0.4770132598956357, 0.04...   \n",
       "6896  [0.3445347175467952, -0.49345817219713883, 0.0...   \n",
       "6897  [5.587042723074906, -3.0189127373273474, -0.69...   \n",
       "6898  [1.8348212505816772, -2.5021137033708087, 0.45...   \n",
       "6899  [1.0371355773081117, -0.8878316745599937, -0.0...   \n",
       "\n",
       "                                         w2v_embeddings  \n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...  \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...  \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...  \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...  \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...  \n",
       "...                                                 ...  \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...  \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...  \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...  \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...  \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...  \n",
       "\n",
       "[6900 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = test_dataset[\"train\"].to_pandas()\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855607fb",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d074645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop nulls\n",
    "# test_df.dropna(axis=0, inplace=True)\n",
    "# #drop duplicates\n",
    "# test_df.drop_duplicates(inplace=True)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810185e2",
   "metadata": {},
   "source": [
    "### Convert to stylo features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9676629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(\"../feat_eng\"))\n",
    "\n",
    "# from feat_eng import has_quotes, has_url, percent_uppercase, frequency_punctuation, percent_whitespace, frequency_words_length, avg_sentence_length\n",
    "\n",
    "\n",
    "\n",
    "# def extract_stylometric_features(example):\n",
    "#     text = example['text']\n",
    "    \n",
    "#     if text == None:\n",
    "#         return{\n",
    "#             \"label\": example.get(\"label\"),\n",
    "#             \"has_quotes\": 0,\n",
    "#             \"has_url\": 0,\n",
    "#             \"percent_uppercase\": 0.0,\n",
    "#             \"frequency_punctuation\": 0,\n",
    "#             \"percent_whitespace\": 0.0,\n",
    "#             \"frequency_words_length_15\": 0,\n",
    "#             \"frequency_words_length_14\": 0,\n",
    "#             \"frequency_words_length_12\": 0,\n",
    "#             \"frequency_words_length_11\": 0,\n",
    "#             \"avg_sentence_length\": 0.0\n",
    "#         }\n",
    "    \n",
    "#     return {\n",
    "#         \"label\": example.get(\"label\"),\n",
    "#         \"has_quotes\" : has_quotes(text),\n",
    "#         \"has_url\": has_url(text),\n",
    "#         \"percent_uppercase\": percent_uppercase(text),\n",
    "#         \"frequency_punctuation\": frequency_punctuation(text),\n",
    "#         \"percent_whitespace\": percent_whitespace(text),\n",
    "#         \"frequency_words_length_15\": frequency_words_length(text, 15),\n",
    "#         \"frequency_words_length_14\": frequency_words_length(text, 14),\n",
    "#         \"frequency_words_length_12\": frequency_words_length(text, 12),\n",
    "#         \"frequency_words_length_11\": frequency_words_length(text, 11),\n",
    "#         \"avg_sentence_length\": avg_sentence_length(text)  \n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea3009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# #change dataset back to huggingface form\n",
    "\n",
    "\n",
    "# test_dataset = Dataset.from_pandas(test_df)\n",
    "# stylo_test = test_dataset.map(extract_stylometric_features)\n",
    "\n",
    "# # stylo_test = stylo_test.remove_columns(['title', 'roberta_embedding', 'bow_embedding', 'tfidf_embedding', 'w2v_embedding'])\n",
    "# print(stylo_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0dee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Push to hub\n",
    "# stylo_test.push_to_hub(\"lelexuanzz/Gossipcop_Politifact_Test_Stylo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edff1e5",
   "metadata": {},
   "source": [
    "## Vector Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16f0e",
   "metadata": {},
   "source": [
    "### BOW-TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0594acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 1169)\n"
     ]
    }
   ],
   "source": [
    "#Tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.05, max_df=0.95, stop_words='english', ngram_range=(1,3))\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(test_df[\"text\"])\n",
    "print(tfidf_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79b221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 651)\t0.17095999921182603\n",
      "  (0, 898)\t0.08547999960591302\n",
      "  (0, 488)\t0.18282260236668418\n",
      "  (0, 410)\t0.25094353635864913\n",
      "  (0, 987)\t0.08752737992315925\n",
      "  (0, 481)\t0.12926393340734607\n",
      "  (0, 884)\t0.09363649008840541\n",
      "  (0, 531)\t0.16655547350541117\n",
      "  (0, 930)\t0.08363177494944153\n",
      "  (0, 1093)\t0.19299719617363553\n",
      "  (0, 70)\t0.10572111171340268\n",
      "  (0, 1027)\t0.2501857363995298\n",
      "  (0, 397)\t0.047357184997965776\n",
      "  (0, 379)\t0.046229405877630685\n",
      "  (0, 196)\t0.04711218863007459\n",
      "  (0, 373)\t0.2883236599478017\n",
      "  (0, 813)\t0.0596010690357604\n",
      "  (0, 1014)\t0.04779232965612915\n",
      "  (0, 720)\t0.09775046970425214\n",
      "  (0, 43)\t0.0978286176308111\n",
      "  (0, 803)\t0.10831420774686594\n",
      "  (0, 331)\t0.0956263254821396\n",
      "  (0, 1083)\t0.0927461442930606\n",
      "  (0, 3)\t0.07012585544380184\n",
      "  (0, 1091)\t0.10640651716020913\n",
      "  :\t:\n",
      "  (6899, 930)\t0.07211581711202476\n",
      "  (6899, 1064)\t0.09382929076926043\n",
      "  (6899, 259)\t0.21210822357526615\n",
      "  (6899, 696)\t0.19807729037440963\n",
      "  (6899, 934)\t0.10303261586779623\n",
      "  (6899, 746)\t0.19555060610158467\n",
      "  (6899, 1024)\t0.21539330008668706\n",
      "  (6899, 1060)\t0.12914878760743315\n",
      "  (6899, 1111)\t0.13487754996825832\n",
      "  (6899, 626)\t0.15344994526769568\n",
      "  (6899, 673)\t0.20606663818799495\n",
      "  (6899, 133)\t0.1703701139640741\n",
      "  (6899, 747)\t0.19036433022262145\n",
      "  (6899, 941)\t0.14792206666680102\n",
      "  (6899, 258)\t0.1985966447582621\n",
      "  (6899, 1156)\t0.1861176298738051\n",
      "  (6899, 878)\t0.27877605943324074\n",
      "  (6899, 1011)\t0.17550906361282298\n",
      "  (6899, 604)\t0.1491997012233373\n",
      "  (6899, 327)\t0.17354276978168337\n",
      "  (6899, 946)\t0.22118168688278944\n",
      "  (6899, 369)\t0.426907189093387\n",
      "  (6899, 940)\t0.19099825650716565\n",
      "  (6899, 425)\t0.2107945113479032\n",
      "  (6899, 1009)\t0.21581786634308212\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d158c63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 300)\n",
      "[[ 3.22305910e-01 -2.83119499e-02  5.49363491e-01 ... -2.65537990e-02\n",
      "   3.65719497e-02  4.78281047e-02]\n",
      " [ 3.08578306e-01  1.19417193e-01  4.53342969e-02 ... -1.15097457e-02\n",
      "   3.49680072e-02  2.44266409e-02]\n",
      " [ 4.23304385e-01  7.86274095e-02  5.77382921e-02 ... -2.39881400e-02\n",
      "   1.95589626e-02 -3.41128586e-02]\n",
      " ...\n",
      " [ 3.00126854e-01 -2.62906111e-01 -8.32331957e-02 ...  1.83975537e-02\n",
      "   3.69450673e-02 -2.74397293e-04]\n",
      " [ 1.85983759e-01 -2.66019185e-01 -9.32809604e-02 ...  2.11396563e-02\n",
      "  -1.04245299e-02 -2.02165002e-02]\n",
      " [ 1.41732095e-01 -1.63308575e-01 -6.91962399e-02 ...  1.08238266e-02\n",
      "  -3.30433969e-02 -3.59793545e-02]]\n"
     ]
    }
   ],
   "source": [
    "#use truncated svd to reduce dimensions of tfidf embeddings\n",
    "#instead of PCA as tfidf embeddings are sparse\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, random_state=45)\n",
    "tfidf_reduced = svd.fit_transform(tfidf_embeddings)\n",
    "print(tfidf_reduced.shape)\n",
    "print(tfidf_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2267b5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf_embedding</th>\n",
       "      <th>bow_embedding</th>\n",
       "      <th>w2v_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The media reported on “5 TikTok dances you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32230591024075883, -0.028311949900530733, 0...</td>\n",
       "      <td>[4.111553694916509, -1.409115414997175, -2.222...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Over 240,000 'unverified' ballots have alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3085783060828623, 0.11941719268786223, 0.04...</td>\n",
       "      <td>[15.340171294779946, 0.6782201912562608, -3.95...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says \"Ron Johnson is making excuses for rioter...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.42330438481857957, 0.07862740950249068, 0.0...</td>\n",
       "      <td>[17.57951108926487, -3.370470538016004, -6.032...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We have seen over the last 10 years ... under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3235193218432473, 0.1435034790001879, -0.01...</td>\n",
       "      <td>[13.623137823124136, 2.594215458879296, 1.1975...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t get involved in the hiring and firing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21714023268741092, -0.013357531354942328, 0...</td>\n",
       "      <td>[11.715673205868844, -3.897763995131928, -0.56...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>Wedding Album: Dancing with the Stars Pro Lind...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14163919484467705, -0.1871535330863532, -0....</td>\n",
       "      <td>[0.5081918596591114, -0.4770132598956357, 0.04...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>WATCH: Sneak Peek: Arizona's Furious Alex Atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10089287837516506, -0.15370171061200275, -0...</td>\n",
       "      <td>[0.3445347175467952, -0.49345817219713883, 0.0...</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Mary Kay Letourneau 'Hopeful' She Can Fix Marr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.30012685395650657, -0.2629061112392972, -0....</td>\n",
       "      <td>[5.587042723074906, -3.0189127373273474, -0.69...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Charlize Theron still upset Aeon Flux didn’t w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18598375894521132, -0.26601918530104635, -0...</td>\n",
       "      <td>[1.8348212505816772, -2.5021137033708087, 0.45...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Debbie Gibson Says ‘DWTS’ Was a Healing Experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14173209453104368, -0.1633085748664822, -0....</td>\n",
       "      <td>[1.0371355773081117, -0.8878316745599937, -0.0...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6900 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     The media reported on “5 TikTok dances you can...      0   \n",
       "1     \"Over 240,000 'unverified' ballots have alread...      0   \n",
       "2     Says \"Ron Johnson is making excuses for rioter...      1   \n",
       "3     “We have seen over the last 10 years ... under...      0   \n",
       "4     “I don’t get involved in the hiring and firing...      0   \n",
       "...                                                 ...    ...   \n",
       "6895  Wedding Album: Dancing with the Stars Pro Lind...      0   \n",
       "6896  WATCH: Sneak Peek: Arizona's Furious Alex Atta...      0   \n",
       "6897  Mary Kay Letourneau 'Hopeful' She Can Fix Marr...      0   \n",
       "6898  Charlize Theron still upset Aeon Flux didn’t w...      0   \n",
       "6899  Debbie Gibson Says ‘DWTS’ Was a Healing Experi...      0   \n",
       "\n",
       "                                        tfidf_embedding  \\\n",
       "0     [0.32230591024075883, -0.028311949900530733, 0...   \n",
       "1     [0.3085783060828623, 0.11941719268786223, 0.04...   \n",
       "2     [0.42330438481857957, 0.07862740950249068, 0.0...   \n",
       "3     [0.3235193218432473, 0.1435034790001879, -0.01...   \n",
       "4     [0.21714023268741092, -0.013357531354942328, 0...   \n",
       "...                                                 ...   \n",
       "6895  [0.14163919484467705, -0.1871535330863532, -0....   \n",
       "6896  [0.10089287837516506, -0.15370171061200275, -0...   \n",
       "6897  [0.30012685395650657, -0.2629061112392972, -0....   \n",
       "6898  [0.18598375894521132, -0.26601918530104635, -0...   \n",
       "6899  [0.14173209453104368, -0.1633085748664822, -0....   \n",
       "\n",
       "                                          bow_embedding  \\\n",
       "0     [4.111553694916509, -1.409115414997175, -2.222...   \n",
       "1     [15.340171294779946, 0.6782201912562608, -3.95...   \n",
       "2     [17.57951108926487, -3.370470538016004, -6.032...   \n",
       "3     [13.623137823124136, 2.594215458879296, 1.1975...   \n",
       "4     [11.715673205868844, -3.897763995131928, -0.56...   \n",
       "...                                                 ...   \n",
       "6895  [0.5081918596591114, -0.4770132598956357, 0.04...   \n",
       "6896  [0.3445347175467952, -0.49345817219713883, 0.0...   \n",
       "6897  [5.587042723074906, -3.0189127373273474, -0.69...   \n",
       "6898  [1.8348212505816772, -2.5021137033708087, 0.45...   \n",
       "6899  [1.0371355773081117, -0.8878316745599937, -0.0...   \n",
       "\n",
       "                                         w2v_embeddings  \n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...  \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...  \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...  \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...  \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...  \n",
       "...                                                 ...  \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...  \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...  \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...  \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...  \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...  \n",
       "\n",
       "[6900 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a new column to the DataFrame\n",
    "test_df['tfidf_embedding'] = list(tfidf_reduced)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca012cf9",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3232a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 1169)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(min_df=0.05, max_df=0.95, stop_words='english', ngram_range=(1,3))\n",
    "bow_embeddings = bow_vectorizer.fit_transform(test_df[\"text\"])\n",
    "print(bow_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d5636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 651)\t2\n",
      "  (0, 898)\t1\n",
      "  (0, 488)\t2\n",
      "  (0, 410)\t2\n",
      "  (0, 987)\t1\n",
      "  (0, 481)\t1\n",
      "  (0, 884)\t1\n",
      "  (0, 531)\t3\n",
      "  (0, 930)\t2\n",
      "  (0, 1093)\t2\n",
      "  (0, 70)\t1\n",
      "  (0, 1027)\t3\n",
      "  (0, 397)\t1\n",
      "  (0, 379)\t1\n",
      "  (0, 196)\t1\n",
      "  (0, 373)\t4\n",
      "  (0, 813)\t1\n",
      "  (0, 1014)\t1\n",
      "  (0, 720)\t2\n",
      "  (0, 43)\t2\n",
      "  (0, 803)\t2\n",
      "  (0, 331)\t1\n",
      "  (0, 1083)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1091)\t1\n",
      "  :\t:\n",
      "  (6899, 930)\t1\n",
      "  (6899, 1064)\t1\n",
      "  (6899, 259)\t1\n",
      "  (6899, 696)\t1\n",
      "  (6899, 934)\t1\n",
      "  (6899, 746)\t1\n",
      "  (6899, 1024)\t1\n",
      "  (6899, 1060)\t1\n",
      "  (6899, 1111)\t1\n",
      "  (6899, 626)\t1\n",
      "  (6899, 673)\t1\n",
      "  (6899, 133)\t1\n",
      "  (6899, 747)\t1\n",
      "  (6899, 941)\t1\n",
      "  (6899, 258)\t1\n",
      "  (6899, 1156)\t1\n",
      "  (6899, 878)\t2\n",
      "  (6899, 1011)\t1\n",
      "  (6899, 604)\t1\n",
      "  (6899, 327)\t1\n",
      "  (6899, 946)\t1\n",
      "  (6899, 369)\t2\n",
      "  (6899, 940)\t1\n",
      "  (6899, 425)\t1\n",
      "  (6899, 1009)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edab4dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 300)\n",
      "[[ 4.11155369 -1.40911541 -2.22285019 ... -0.05428714 -0.12782605\n",
      "   0.2925218 ]\n",
      " [15.34017129  0.67822019 -3.95217032 ...  1.19092561  1.43619475\n",
      "  -1.39075334]\n",
      " [17.57951109 -3.37047054 -6.03270616 ...  0.35279308 -0.41493208\n",
      "   0.11685365]\n",
      " ...\n",
      " [ 5.58704272 -3.01891274 -0.69692462 ... -0.78713293 -0.20372279\n",
      "  -1.05756739]\n",
      " [ 1.83482125 -2.5021137   0.45429264 ... -0.1381487  -0.09905513\n",
      "  -0.19622964]\n",
      " [ 1.03713558 -0.88783167 -0.09315308 ... -0.14861279  0.0564323\n",
      "   0.29132153]]\n"
     ]
    }
   ],
   "source": [
    "bow_reduced = svd.fit_transform(bow_embeddings)\n",
    "print(bow_reduced.shape)\n",
    "print(bow_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17ef20aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf_embedding</th>\n",
       "      <th>bow_embedding</th>\n",
       "      <th>w2v_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The media reported on “5 TikTok dances you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32230591024075883, -0.028311949900530733, 0...</td>\n",
       "      <td>[4.111553694916485, -1.4091154149948346, -2.22...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Over 240,000 'unverified' ballots have alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3085783060828623, 0.11941719268786223, 0.04...</td>\n",
       "      <td>[15.340171294779958, 0.6782201913070947, -3.95...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says \"Ron Johnson is making excuses for rioter...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.42330438481857957, 0.07862740950249068, 0.0...</td>\n",
       "      <td>[17.579511089264834, -3.370470538008803, -6.03...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We have seen over the last 10 years ... under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3235193218432473, 0.1435034790001879, -0.01...</td>\n",
       "      <td>[13.623137823124184, 2.594215458815296, 1.1975...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t get involved in the hiring and firing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21714023268741092, -0.013357531354942328, 0...</td>\n",
       "      <td>[11.715673205868814, -3.8977639951627094, -0.5...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>Wedding Album: Dancing with the Stars Pro Lind...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14163919484467705, -0.1871535330863532, -0....</td>\n",
       "      <td>[0.5081918596591085, -0.4770132598925509, 0.04...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>WATCH: Sneak Peek: Arizona's Furious Alex Atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10089287837516506, -0.15370171061200275, -0...</td>\n",
       "      <td>[0.34453471754679216, -0.49345817220335975, 0....</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Mary Kay Letourneau 'Hopeful' She Can Fix Marr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.30012685395650657, -0.2629061112392972, -0....</td>\n",
       "      <td>[5.587042723074881, -3.018912737323734, -0.696...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Charlize Theron still upset Aeon Flux didn’t w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18598375894521132, -0.26601918530104635, -0...</td>\n",
       "      <td>[1.8348212505816643, -2.5021137033474385, 0.45...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Debbie Gibson Says ‘DWTS’ Was a Healing Experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14173209453104368, -0.1633085748664822, -0....</td>\n",
       "      <td>[1.0371355773081052, -0.8878316745591224, -0.0...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6900 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     The media reported on “5 TikTok dances you can...      0   \n",
       "1     \"Over 240,000 'unverified' ballots have alread...      0   \n",
       "2     Says \"Ron Johnson is making excuses for rioter...      1   \n",
       "3     “We have seen over the last 10 years ... under...      0   \n",
       "4     “I don’t get involved in the hiring and firing...      0   \n",
       "...                                                 ...    ...   \n",
       "6895  Wedding Album: Dancing with the Stars Pro Lind...      0   \n",
       "6896  WATCH: Sneak Peek: Arizona's Furious Alex Atta...      0   \n",
       "6897  Mary Kay Letourneau 'Hopeful' She Can Fix Marr...      0   \n",
       "6898  Charlize Theron still upset Aeon Flux didn’t w...      0   \n",
       "6899  Debbie Gibson Says ‘DWTS’ Was a Healing Experi...      0   \n",
       "\n",
       "                                        tfidf_embedding  \\\n",
       "0     [0.32230591024075883, -0.028311949900530733, 0...   \n",
       "1     [0.3085783060828623, 0.11941719268786223, 0.04...   \n",
       "2     [0.42330438481857957, 0.07862740950249068, 0.0...   \n",
       "3     [0.3235193218432473, 0.1435034790001879, -0.01...   \n",
       "4     [0.21714023268741092, -0.013357531354942328, 0...   \n",
       "...                                                 ...   \n",
       "6895  [0.14163919484467705, -0.1871535330863532, -0....   \n",
       "6896  [0.10089287837516506, -0.15370171061200275, -0...   \n",
       "6897  [0.30012685395650657, -0.2629061112392972, -0....   \n",
       "6898  [0.18598375894521132, -0.26601918530104635, -0...   \n",
       "6899  [0.14173209453104368, -0.1633085748664822, -0....   \n",
       "\n",
       "                                          bow_embedding  \\\n",
       "0     [4.111553694916485, -1.4091154149948346, -2.22...   \n",
       "1     [15.340171294779958, 0.6782201913070947, -3.95...   \n",
       "2     [17.579511089264834, -3.370470538008803, -6.03...   \n",
       "3     [13.623137823124184, 2.594215458815296, 1.1975...   \n",
       "4     [11.715673205868814, -3.8977639951627094, -0.5...   \n",
       "...                                                 ...   \n",
       "6895  [0.5081918596591085, -0.4770132598925509, 0.04...   \n",
       "6896  [0.34453471754679216, -0.49345817220335975, 0....   \n",
       "6897  [5.587042723074881, -3.018912737323734, -0.696...   \n",
       "6898  [1.8348212505816643, -2.5021137033474385, 0.45...   \n",
       "6899  [1.0371355773081052, -0.8878316745591224, -0.0...   \n",
       "\n",
       "                                         w2v_embeddings  \n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...  \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...  \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...  \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...  \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...  \n",
       "...                                                 ...  \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...  \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...  \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...  \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...  \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...  \n",
       "\n",
       "[6900 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a new column to the DataFrame\n",
    "test_df['bow_embedding'] = list(bow_reduced)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d8d77",
   "metadata": {},
   "source": [
    "### CBOW-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54cb52e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Lee Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\Lee\n",
      "[nltk_data]    |     Le Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Lee Le\n",
      "[nltk_data]    |     Xuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "nltk.download('all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6728734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = test_df[\"text\"].astype(str).tolist()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in ENGLISH_STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "tokenized_texts = [preprocess(t) for t in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a725ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "\n",
    "#load pretrained model\n",
    "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97cecf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get top 10,000 most frequent words in corpus\n",
    "# from collections import Counter\n",
    "\n",
    "# word_freq = Counter([word for tokens in tokenized_texts for word in tokens])\n",
    "# vocab = [word for word, _ in word_freq.most_common(10000)]\n",
    "# word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47e5f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = w2v_vectors.vector_size\n",
    "num_docs = len(tokenized_texts)\n",
    "\n",
    "w2v_embeddings = np.zeros((num_docs, embedding_dim), dtype=np.float32)\n",
    "\n",
    "for i, doc in enumerate(tokenized_texts):\n",
    "    vectors = [w2v_vectors[word] for word in doc if word in w2v_vectors]\n",
    "    if vectors:\n",
    "        w2v_embeddings[i] = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        w2v_embeddings[i] = np.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2018f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf_embedding</th>\n",
       "      <th>bow_embedding</th>\n",
       "      <th>w2v_embeddings</th>\n",
       "      <th>w2v_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The media reported on “5 TikTok dances you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32230591024075883, -0.028311949900530733, 0...</td>\n",
       "      <td>[4.111553694916485, -1.4091154149948346, -2.22...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Over 240,000 'unverified' ballots have alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3085783060828623, 0.11941719268786223, 0.04...</td>\n",
       "      <td>[15.340171294779958, 0.6782201913070947, -3.95...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says \"Ron Johnson is making excuses for rioter...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.42330438481857957, 0.07862740950249068, 0.0...</td>\n",
       "      <td>[17.579511089264834, -3.370470538008803, -6.03...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We have seen over the last 10 years ... under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3235193218432473, 0.1435034790001879, -0.01...</td>\n",
       "      <td>[13.623137823124184, 2.594215458815296, 1.1975...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t get involved in the hiring and firing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21714023268741092, -0.013357531354942328, 0...</td>\n",
       "      <td>[11.715673205868814, -3.8977639951627094, -0.5...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>Wedding Album: Dancing with the Stars Pro Lind...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14163919484467705, -0.1871535330863532, -0....</td>\n",
       "      <td>[0.5081918596591085, -0.4770132598925509, 0.04...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>WATCH: Sneak Peek: Arizona's Furious Alex Atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10089287837516506, -0.15370171061200275, -0...</td>\n",
       "      <td>[0.34453471754679216, -0.49345817220335975, 0....</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Mary Kay Letourneau 'Hopeful' She Can Fix Marr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.30012685395650657, -0.2629061112392972, -0....</td>\n",
       "      <td>[5.587042723074881, -3.018912737323734, -0.696...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Charlize Theron still upset Aeon Flux didn’t w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18598375894521132, -0.26601918530104635, -0...</td>\n",
       "      <td>[1.8348212505816643, -2.5021137033474385, 0.45...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Debbie Gibson Says ‘DWTS’ Was a Healing Experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14173209453104368, -0.1633085748664822, -0....</td>\n",
       "      <td>[1.0371355773081052, -0.8878316745591224, -0.0...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     The media reported on “5 TikTok dances you can...      0   \n",
       "1     \"Over 240,000 'unverified' ballots have alread...      0   \n",
       "2     Says \"Ron Johnson is making excuses for rioter...      1   \n",
       "3     “We have seen over the last 10 years ... under...      0   \n",
       "4     “I don’t get involved in the hiring and firing...      0   \n",
       "...                                                 ...    ...   \n",
       "6895  Wedding Album: Dancing with the Stars Pro Lind...      0   \n",
       "6896  WATCH: Sneak Peek: Arizona's Furious Alex Atta...      0   \n",
       "6897  Mary Kay Letourneau 'Hopeful' She Can Fix Marr...      0   \n",
       "6898  Charlize Theron still upset Aeon Flux didn’t w...      0   \n",
       "6899  Debbie Gibson Says ‘DWTS’ Was a Healing Experi...      0   \n",
       "\n",
       "                                        tfidf_embedding  \\\n",
       "0     [0.32230591024075883, -0.028311949900530733, 0...   \n",
       "1     [0.3085783060828623, 0.11941719268786223, 0.04...   \n",
       "2     [0.42330438481857957, 0.07862740950249068, 0.0...   \n",
       "3     [0.3235193218432473, 0.1435034790001879, -0.01...   \n",
       "4     [0.21714023268741092, -0.013357531354942328, 0...   \n",
       "...                                                 ...   \n",
       "6895  [0.14163919484467705, -0.1871535330863532, -0....   \n",
       "6896  [0.10089287837516506, -0.15370171061200275, -0...   \n",
       "6897  [0.30012685395650657, -0.2629061112392972, -0....   \n",
       "6898  [0.18598375894521132, -0.26601918530104635, -0...   \n",
       "6899  [0.14173209453104368, -0.1633085748664822, -0....   \n",
       "\n",
       "                                          bow_embedding  \\\n",
       "0     [4.111553694916485, -1.4091154149948346, -2.22...   \n",
       "1     [15.340171294779958, 0.6782201913070947, -3.95...   \n",
       "2     [17.579511089264834, -3.370470538008803, -6.03...   \n",
       "3     [13.623137823124184, 2.594215458815296, 1.1975...   \n",
       "4     [11.715673205868814, -3.8977639951627094, -0.5...   \n",
       "...                                                 ...   \n",
       "6895  [0.5081918596591085, -0.4770132598925509, 0.04...   \n",
       "6896  [0.34453471754679216, -0.49345817220335975, 0....   \n",
       "6897  [5.587042723074881, -3.018912737323734, -0.696...   \n",
       "6898  [1.8348212505816643, -2.5021137033474385, 0.45...   \n",
       "6899  [1.0371355773081052, -0.8878316745591224, -0.0...   \n",
       "\n",
       "                                         w2v_embeddings  \\\n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...   \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...   \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...   \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...   \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...   \n",
       "...                                                 ...   \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...   \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...   \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...   \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...   \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...   \n",
       "\n",
       "                                          w2v_embedding  \n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...  \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...  \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...  \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...  \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...  \n",
       "...                                                 ...  \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...  \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...  \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...  \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...  \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...  \n",
       "\n",
       "[6900 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(w2v_embeddings.shape)\n",
    "test_df['w2v_embedding'] = list(w2v_embeddings)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f4689",
   "metadata": {},
   "source": [
    "### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "232ee49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "roberta_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41b381f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roberta_embedding(text):\n",
    "    inputs = roberta_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77f0b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed RoBERTa batch 1/216\n",
      "Processed RoBERTa batch 2/216\n",
      "Processed RoBERTa batch 3/216\n",
      "Processed RoBERTa batch 4/216\n",
      "Processed RoBERTa batch 5/216\n",
      "Processed RoBERTa batch 6/216\n",
      "Processed RoBERTa batch 7/216\n",
      "Processed RoBERTa batch 8/216\n",
      "Processed RoBERTa batch 9/216\n",
      "Processed RoBERTa batch 10/216\n",
      "Processed RoBERTa batch 11/216\n",
      "Processed RoBERTa batch 12/216\n",
      "Processed RoBERTa batch 13/216\n",
      "Processed RoBERTa batch 14/216\n",
      "Processed RoBERTa batch 15/216\n",
      "Processed RoBERTa batch 16/216\n",
      "Processed RoBERTa batch 17/216\n",
      "Processed RoBERTa batch 18/216\n",
      "Processed RoBERTa batch 19/216\n",
      "Processed RoBERTa batch 20/216\n",
      "Processed RoBERTa batch 21/216\n",
      "Processed RoBERTa batch 22/216\n",
      "Processed RoBERTa batch 23/216\n",
      "Processed RoBERTa batch 24/216\n",
      "Processed RoBERTa batch 25/216\n",
      "Processed RoBERTa batch 26/216\n",
      "Processed RoBERTa batch 27/216\n",
      "Processed RoBERTa batch 28/216\n",
      "Processed RoBERTa batch 29/216\n",
      "Processed RoBERTa batch 30/216\n",
      "Processed RoBERTa batch 31/216\n",
      "Processed RoBERTa batch 32/216\n",
      "Processed RoBERTa batch 33/216\n",
      "Processed RoBERTa batch 34/216\n",
      "Processed RoBERTa batch 35/216\n",
      "Processed RoBERTa batch 36/216\n",
      "Processed RoBERTa batch 37/216\n",
      "Processed RoBERTa batch 38/216\n",
      "Processed RoBERTa batch 39/216\n",
      "Processed RoBERTa batch 40/216\n",
      "Processed RoBERTa batch 41/216\n",
      "Processed RoBERTa batch 42/216\n",
      "Processed RoBERTa batch 43/216\n",
      "Processed RoBERTa batch 44/216\n",
      "Processed RoBERTa batch 45/216\n",
      "Processed RoBERTa batch 46/216\n",
      "Processed RoBERTa batch 47/216\n",
      "Processed RoBERTa batch 48/216\n",
      "Processed RoBERTa batch 49/216\n",
      "Processed RoBERTa batch 50/216\n",
      "Processed RoBERTa batch 51/216\n",
      "Processed RoBERTa batch 52/216\n",
      "Processed RoBERTa batch 53/216\n",
      "Processed RoBERTa batch 54/216\n",
      "Processed RoBERTa batch 55/216\n",
      "Processed RoBERTa batch 56/216\n",
      "Processed RoBERTa batch 57/216\n",
      "Processed RoBERTa batch 58/216\n",
      "Processed RoBERTa batch 59/216\n",
      "Processed RoBERTa batch 60/216\n",
      "Processed RoBERTa batch 61/216\n",
      "Processed RoBERTa batch 62/216\n",
      "Processed RoBERTa batch 63/216\n",
      "Processed RoBERTa batch 64/216\n",
      "Processed RoBERTa batch 65/216\n",
      "Processed RoBERTa batch 66/216\n",
      "Processed RoBERTa batch 67/216\n",
      "Processed RoBERTa batch 68/216\n",
      "Processed RoBERTa batch 69/216\n",
      "Processed RoBERTa batch 70/216\n",
      "Processed RoBERTa batch 71/216\n",
      "Processed RoBERTa batch 72/216\n",
      "Processed RoBERTa batch 73/216\n",
      "Processed RoBERTa batch 74/216\n",
      "Processed RoBERTa batch 75/216\n",
      "Processed RoBERTa batch 76/216\n",
      "Processed RoBERTa batch 77/216\n",
      "Processed RoBERTa batch 78/216\n",
      "Processed RoBERTa batch 79/216\n",
      "Processed RoBERTa batch 80/216\n",
      "Processed RoBERTa batch 81/216\n",
      "Processed RoBERTa batch 82/216\n",
      "Processed RoBERTa batch 83/216\n",
      "Processed RoBERTa batch 84/216\n",
      "Processed RoBERTa batch 85/216\n",
      "Processed RoBERTa batch 86/216\n",
      "Processed RoBERTa batch 87/216\n",
      "Processed RoBERTa batch 88/216\n",
      "Processed RoBERTa batch 89/216\n",
      "Processed RoBERTa batch 90/216\n",
      "Processed RoBERTa batch 91/216\n",
      "Processed RoBERTa batch 92/216\n",
      "Processed RoBERTa batch 93/216\n",
      "Processed RoBERTa batch 94/216\n",
      "Processed RoBERTa batch 95/216\n",
      "Processed RoBERTa batch 96/216\n",
      "Processed RoBERTa batch 97/216\n",
      "Processed RoBERTa batch 98/216\n",
      "Processed RoBERTa batch 99/216\n",
      "Processed RoBERTa batch 100/216\n",
      "Processed RoBERTa batch 101/216\n",
      "Processed RoBERTa batch 102/216\n",
      "Processed RoBERTa batch 103/216\n",
      "Processed RoBERTa batch 104/216\n",
      "Processed RoBERTa batch 105/216\n",
      "Processed RoBERTa batch 106/216\n",
      "Processed RoBERTa batch 107/216\n",
      "Processed RoBERTa batch 108/216\n",
      "Processed RoBERTa batch 109/216\n",
      "Processed RoBERTa batch 110/216\n",
      "Processed RoBERTa batch 111/216\n",
      "Processed RoBERTa batch 112/216\n",
      "Processed RoBERTa batch 113/216\n",
      "Processed RoBERTa batch 114/216\n",
      "Processed RoBERTa batch 115/216\n",
      "Processed RoBERTa batch 116/216\n",
      "Processed RoBERTa batch 117/216\n",
      "Processed RoBERTa batch 118/216\n",
      "Processed RoBERTa batch 119/216\n",
      "Processed RoBERTa batch 120/216\n",
      "Processed RoBERTa batch 121/216\n",
      "Processed RoBERTa batch 122/216\n",
      "Processed RoBERTa batch 123/216\n",
      "Processed RoBERTa batch 124/216\n",
      "Processed RoBERTa batch 125/216\n",
      "Processed RoBERTa batch 126/216\n",
      "Processed RoBERTa batch 127/216\n",
      "Processed RoBERTa batch 128/216\n",
      "Processed RoBERTa batch 129/216\n",
      "Processed RoBERTa batch 130/216\n",
      "Processed RoBERTa batch 131/216\n",
      "Processed RoBERTa batch 132/216\n",
      "Processed RoBERTa batch 133/216\n",
      "Processed RoBERTa batch 134/216\n",
      "Processed RoBERTa batch 135/216\n",
      "Processed RoBERTa batch 136/216\n",
      "Processed RoBERTa batch 137/216\n",
      "Processed RoBERTa batch 138/216\n",
      "Processed RoBERTa batch 139/216\n",
      "Processed RoBERTa batch 140/216\n",
      "Processed RoBERTa batch 141/216\n",
      "Processed RoBERTa batch 142/216\n",
      "Processed RoBERTa batch 143/216\n",
      "Processed RoBERTa batch 144/216\n",
      "Processed RoBERTa batch 145/216\n",
      "Processed RoBERTa batch 146/216\n",
      "Processed RoBERTa batch 147/216\n",
      "Processed RoBERTa batch 148/216\n",
      "Processed RoBERTa batch 149/216\n",
      "Processed RoBERTa batch 150/216\n",
      "Processed RoBERTa batch 151/216\n",
      "Processed RoBERTa batch 152/216\n",
      "Processed RoBERTa batch 153/216\n",
      "Processed RoBERTa batch 154/216\n",
      "Processed RoBERTa batch 155/216\n",
      "Processed RoBERTa batch 156/216\n",
      "Processed RoBERTa batch 157/216\n",
      "Processed RoBERTa batch 158/216\n",
      "Processed RoBERTa batch 159/216\n",
      "Processed RoBERTa batch 160/216\n",
      "Processed RoBERTa batch 161/216\n",
      "Processed RoBERTa batch 162/216\n",
      "Processed RoBERTa batch 163/216\n",
      "Processed RoBERTa batch 164/216\n",
      "Processed RoBERTa batch 165/216\n",
      "Processed RoBERTa batch 166/216\n",
      "Processed RoBERTa batch 167/216\n",
      "Processed RoBERTa batch 168/216\n",
      "Processed RoBERTa batch 169/216\n",
      "Processed RoBERTa batch 170/216\n",
      "Processed RoBERTa batch 171/216\n",
      "Processed RoBERTa batch 172/216\n",
      "Processed RoBERTa batch 173/216\n",
      "Processed RoBERTa batch 174/216\n",
      "Processed RoBERTa batch 175/216\n",
      "Processed RoBERTa batch 176/216\n",
      "Processed RoBERTa batch 177/216\n",
      "Processed RoBERTa batch 178/216\n",
      "Processed RoBERTa batch 179/216\n",
      "Processed RoBERTa batch 180/216\n",
      "Processed RoBERTa batch 181/216\n",
      "Processed RoBERTa batch 182/216\n",
      "Processed RoBERTa batch 183/216\n",
      "Processed RoBERTa batch 184/216\n",
      "Processed RoBERTa batch 185/216\n",
      "Processed RoBERTa batch 186/216\n",
      "Processed RoBERTa batch 187/216\n",
      "Processed RoBERTa batch 188/216\n",
      "Processed RoBERTa batch 189/216\n",
      "Processed RoBERTa batch 190/216\n",
      "Processed RoBERTa batch 191/216\n",
      "Processed RoBERTa batch 192/216\n",
      "Processed RoBERTa batch 193/216\n",
      "Processed RoBERTa batch 194/216\n",
      "Processed RoBERTa batch 195/216\n",
      "Processed RoBERTa batch 196/216\n",
      "Processed RoBERTa batch 197/216\n",
      "Processed RoBERTa batch 198/216\n",
      "Processed RoBERTa batch 199/216\n",
      "Processed RoBERTa batch 200/216\n",
      "Processed RoBERTa batch 201/216\n",
      "Processed RoBERTa batch 202/216\n",
      "Processed RoBERTa batch 203/216\n",
      "Processed RoBERTa batch 204/216\n",
      "Processed RoBERTa batch 205/216\n",
      "Processed RoBERTa batch 206/216\n",
      "Processed RoBERTa batch 207/216\n",
      "Processed RoBERTa batch 208/216\n",
      "Processed RoBERTa batch 209/216\n",
      "Processed RoBERTa batch 210/216\n",
      "Processed RoBERTa batch 211/216\n",
      "Processed RoBERTa batch 212/216\n",
      "Processed RoBERTa batch 213/216\n",
      "Processed RoBERTa batch 214/216\n",
      "Processed RoBERTa batch 215/216\n",
      "Processed RoBERTa batch 216/216\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "roberta_embeddings = []\n",
    "for i in range(0, len(test_df), batch_size):\n",
    "    batch_texts = test_df['text'][i:i+batch_size].tolist()\n",
    "    batch_embeds = [get_roberta_embedding(text) for text in batch_texts]\n",
    "    roberta_embeddings.extend(batch_embeds)\n",
    "    print(f\"Processed RoBERTa batch {i//batch_size + 1}/{len(test_df)//batch_size + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa374599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 768)\n"
     ]
    }
   ],
   "source": [
    "roberta_embeddings = np.array(roberta_embeddings)\n",
    "print(roberta_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecd2df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6900, 300)\n"
     ]
    }
   ],
   "source": [
    "#reduce dimensionality with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=300, random_state=45)  \n",
    "roberta_reduced = pca.fit_transform(roberta_embeddings)\n",
    "print(roberta_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "669e174f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf_embedding</th>\n",
       "      <th>bow_embedding</th>\n",
       "      <th>w2v_embeddings</th>\n",
       "      <th>w2v_embedding</th>\n",
       "      <th>roberta_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The media reported on “5 TikTok dances you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32230591024075883, -0.028311949900530733, 0...</td>\n",
       "      <td>[4.111553694916485, -1.4091154149948346, -2.22...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "      <td>[-1.7613875, -0.29602507, 1.0549823, 0.4420643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Over 240,000 'unverified' ballots have alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3085783060828623, 0.11941719268786223, 0.04...</td>\n",
       "      <td>[15.340171294779958, 0.6782201913070947, -3.95...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "      <td>[0.5113651, -0.5632596, -0.08679409, 0.0578204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says \"Ron Johnson is making excuses for rioter...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.42330438481857957, 0.07862740950249068, 0.0...</td>\n",
       "      <td>[17.579511089264834, -3.370470538008803, -6.03...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "      <td>[0.4061544, 0.033350937, -0.30923274, -0.04415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We have seen over the last 10 years ... under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3235193218432473, 0.1435034790001879, -0.01...</td>\n",
       "      <td>[13.623137823124184, 2.594215458815296, 1.1975...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "      <td>[0.74233645, -0.8022974, 0.30516905, -0.150992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t get involved in the hiring and firing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21714023268741092, -0.013357531354942328, 0...</td>\n",
       "      <td>[11.715673205868814, -3.8977639951627094, -0.5...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "      <td>[0.4902299, -0.04709744, -0.11096978, -0.33383...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>Wedding Album: Dancing with the Stars Pro Lind...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14163919484467705, -0.1871535330863532, -0....</td>\n",
       "      <td>[0.5081918596591085, -0.4770132598925509, 0.04...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "      <td>[-4.3445463, -0.82720375, -0.76975673, 0.55827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>WATCH: Sneak Peek: Arizona's Furious Alex Atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10089287837516506, -0.15370171061200275, -0...</td>\n",
       "      <td>[0.34453471754679216, -0.49345817220335975, 0....</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "      <td>[-3.484783, -0.13741834, -0.2212331, 0.3501901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Mary Kay Letourneau 'Hopeful' She Can Fix Marr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.30012685395650657, -0.2629061112392972, -0....</td>\n",
       "      <td>[5.587042723074881, -3.018912737323734, -0.696...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "      <td>[0.060427, 0.93066996, -0.5383188, -0.11787002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Charlize Theron still upset Aeon Flux didn’t w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18598375894521132, -0.26601918530104635, -0...</td>\n",
       "      <td>[1.8348212505816643, -2.5021137033474385, 0.45...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "      <td>[0.29304072, 1.2304765, -0.0635877, 0.6557827,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Debbie Gibson Says ‘DWTS’ Was a Healing Experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14173209453104368, -0.1633085748664822, -0....</td>\n",
       "      <td>[1.0371355773081052, -0.8878316745591224, -0.0...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "      <td>[-2.2836635, -0.034083482, 0.5534697, 1.026466...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6900 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     The media reported on “5 TikTok dances you can...      0   \n",
       "1     \"Over 240,000 'unverified' ballots have alread...      0   \n",
       "2     Says \"Ron Johnson is making excuses for rioter...      1   \n",
       "3     “We have seen over the last 10 years ... under...      0   \n",
       "4     “I don’t get involved in the hiring and firing...      0   \n",
       "...                                                 ...    ...   \n",
       "6895  Wedding Album: Dancing with the Stars Pro Lind...      0   \n",
       "6896  WATCH: Sneak Peek: Arizona's Furious Alex Atta...      0   \n",
       "6897  Mary Kay Letourneau 'Hopeful' She Can Fix Marr...      0   \n",
       "6898  Charlize Theron still upset Aeon Flux didn’t w...      0   \n",
       "6899  Debbie Gibson Says ‘DWTS’ Was a Healing Experi...      0   \n",
       "\n",
       "                                        tfidf_embedding  \\\n",
       "0     [0.32230591024075883, -0.028311949900530733, 0...   \n",
       "1     [0.3085783060828623, 0.11941719268786223, 0.04...   \n",
       "2     [0.42330438481857957, 0.07862740950249068, 0.0...   \n",
       "3     [0.3235193218432473, 0.1435034790001879, -0.01...   \n",
       "4     [0.21714023268741092, -0.013357531354942328, 0...   \n",
       "...                                                 ...   \n",
       "6895  [0.14163919484467705, -0.1871535330863532, -0....   \n",
       "6896  [0.10089287837516506, -0.15370171061200275, -0...   \n",
       "6897  [0.30012685395650657, -0.2629061112392972, -0....   \n",
       "6898  [0.18598375894521132, -0.26601918530104635, -0...   \n",
       "6899  [0.14173209453104368, -0.1633085748664822, -0....   \n",
       "\n",
       "                                          bow_embedding  \\\n",
       "0     [4.111553694916485, -1.4091154149948346, -2.22...   \n",
       "1     [15.340171294779958, 0.6782201913070947, -3.95...   \n",
       "2     [17.579511089264834, -3.370470538008803, -6.03...   \n",
       "3     [13.623137823124184, 2.594215458815296, 1.1975...   \n",
       "4     [11.715673205868814, -3.8977639951627094, -0.5...   \n",
       "...                                                 ...   \n",
       "6895  [0.5081918596591085, -0.4770132598925509, 0.04...   \n",
       "6896  [0.34453471754679216, -0.49345817220335975, 0....   \n",
       "6897  [5.587042723074881, -3.018912737323734, -0.696...   \n",
       "6898  [1.8348212505816643, -2.5021137033474385, 0.45...   \n",
       "6899  [1.0371355773081052, -0.8878316745591224, -0.0...   \n",
       "\n",
       "                                         w2v_embeddings  \\\n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...   \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...   \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...   \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...   \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...   \n",
       "...                                                 ...   \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...   \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...   \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...   \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...   \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...   \n",
       "\n",
       "                                          w2v_embedding  \\\n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...   \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...   \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...   \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...   \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...   \n",
       "...                                                 ...   \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...   \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...   \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...   \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...   \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...   \n",
       "\n",
       "                                      roberta_embedding  \n",
       "0     [-1.7613875, -0.29602507, 1.0549823, 0.4420643...  \n",
       "1     [0.5113651, -0.5632596, -0.08679409, 0.0578204...  \n",
       "2     [0.4061544, 0.033350937, -0.30923274, -0.04415...  \n",
       "3     [0.74233645, -0.8022974, 0.30516905, -0.150992...  \n",
       "4     [0.4902299, -0.04709744, -0.11096978, -0.33383...  \n",
       "...                                                 ...  \n",
       "6895  [-4.3445463, -0.82720375, -0.76975673, 0.55827...  \n",
       "6896  [-3.484783, -0.13741834, -0.2212331, 0.3501901...  \n",
       "6897  [0.060427, 0.93066996, -0.5383188, -0.11787002...  \n",
       "6898  [0.29304072, 1.2304765, -0.0635877, 0.6557827,...  \n",
       "6899  [-2.2836635, -0.034083482, 0.5534697, 1.026466...  \n",
       "\n",
       "[6900 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df['roberta_embedding'] = list(roberta_reduced)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853d202",
   "metadata": {},
   "source": [
    "## Push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c461179b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tfidf_embedding</th>\n",
       "      <th>bow_embedding</th>\n",
       "      <th>w2v_embedding</th>\n",
       "      <th>roberta_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The media reported on “5 TikTok dances you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.32230591024075883, -0.028311949900530733, 0...</td>\n",
       "      <td>[4.111553694916485, -1.4091154149948346, -2.22...</td>\n",
       "      <td>[0.023112578, 0.004341462, -0.028385904, 0.039...</td>\n",
       "      <td>[-1.7613875, -0.29602507, 1.0549823, 0.4420643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Over 240,000 'unverified' ballots have alread...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3085783060828623, 0.11941719268786223, 0.04...</td>\n",
       "      <td>[15.340171294779958, 0.6782201913070947, -3.95...</td>\n",
       "      <td>[0.013536842, 0.009806181, 0.0037024287, 0.046...</td>\n",
       "      <td>[0.5113651, -0.5632596, -0.08679409, 0.0578204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Says \"Ron Johnson is making excuses for rioter...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.42330438481857957, 0.07862740950249068, 0.0...</td>\n",
       "      <td>[17.579511089264834, -3.370470538008803, -6.03...</td>\n",
       "      <td>[0.023063188, 0.059482925, 0.01734151, 0.06674...</td>\n",
       "      <td>[0.4061544, 0.033350937, -0.30923274, -0.04415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“We have seen over the last 10 years ... under...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3235193218432473, 0.1435034790001879, -0.01...</td>\n",
       "      <td>[13.623137823124184, 2.594215458815296, 1.1975...</td>\n",
       "      <td>[-0.03447485, 0.012470036, 0.040506534, 0.1266...</td>\n",
       "      <td>[0.74233645, -0.8022974, 0.30516905, -0.150992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“I don’t get involved in the hiring and firing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.21714023268741092, -0.013357531354942328, 0...</td>\n",
       "      <td>[11.715673205868814, -3.8977639951627094, -0.5...</td>\n",
       "      <td>[-0.039161813, 0.050845865, 0.045167223, -0.00...</td>\n",
       "      <td>[0.4902299, -0.04709744, -0.11096978, -0.33383...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6895</th>\n",
       "      <td>Wedding Album: Dancing with the Stars Pro Lind...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14163919484467705, -0.1871535330863532, -0....</td>\n",
       "      <td>[0.5081918596591085, -0.4770132598925509, 0.04...</td>\n",
       "      <td>[0.007478841, 0.028465271, 0.01225586, 0.05277...</td>\n",
       "      <td>[-4.3445463, -0.82720375, -0.76975673, 0.55827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>WATCH: Sneak Peek: Arizona's Furious Alex Atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.10089287837516506, -0.15370171061200275, -0...</td>\n",
       "      <td>[0.34453471754679216, -0.49345817220335975, 0....</td>\n",
       "      <td>[0.03495687, 0.08366224, 0.03197157, 0.0962388...</td>\n",
       "      <td>[-3.484783, -0.13741834, -0.2212331, 0.3501901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>Mary Kay Letourneau 'Hopeful' She Can Fix Marr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.30012685395650657, -0.2629061112392972, -0....</td>\n",
       "      <td>[5.587042723074881, -3.018912737323734, -0.696...</td>\n",
       "      <td>[0.0066373795, 0.04132683, 0.008552303, 0.0507...</td>\n",
       "      <td>[0.060427, 0.93066996, -0.5383188, -0.11787002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>Charlize Theron still upset Aeon Flux didn’t w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18598375894521132, -0.26601918530104635, -0...</td>\n",
       "      <td>[1.8348212505816643, -2.5021137033474385, 0.45...</td>\n",
       "      <td>[0.032494266, 0.038722403, -0.01410776, 0.0764...</td>\n",
       "      <td>[0.29304072, 1.2304765, -0.0635877, 0.6557827,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>Debbie Gibson Says ‘DWTS’ Was a Healing Experi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14173209453104368, -0.1633085748664822, -0....</td>\n",
       "      <td>[1.0371355773081052, -0.8878316745591224, -0.0...</td>\n",
       "      <td>[-0.034314055, 0.05650489, -0.031183043, 0.083...</td>\n",
       "      <td>[-2.2836635, -0.034083482, 0.5534697, 1.026466...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     The media reported on “5 TikTok dances you can...      0   \n",
       "1     \"Over 240,000 'unverified' ballots have alread...      0   \n",
       "2     Says \"Ron Johnson is making excuses for rioter...      1   \n",
       "3     “We have seen over the last 10 years ... under...      0   \n",
       "4     “I don’t get involved in the hiring and firing...      0   \n",
       "...                                                 ...    ...   \n",
       "6895  Wedding Album: Dancing with the Stars Pro Lind...      0   \n",
       "6896  WATCH: Sneak Peek: Arizona's Furious Alex Atta...      0   \n",
       "6897  Mary Kay Letourneau 'Hopeful' She Can Fix Marr...      0   \n",
       "6898  Charlize Theron still upset Aeon Flux didn’t w...      0   \n",
       "6899  Debbie Gibson Says ‘DWTS’ Was a Healing Experi...      0   \n",
       "\n",
       "                                        tfidf_embedding  \\\n",
       "0     [0.32230591024075883, -0.028311949900530733, 0...   \n",
       "1     [0.3085783060828623, 0.11941719268786223, 0.04...   \n",
       "2     [0.42330438481857957, 0.07862740950249068, 0.0...   \n",
       "3     [0.3235193218432473, 0.1435034790001879, -0.01...   \n",
       "4     [0.21714023268741092, -0.013357531354942328, 0...   \n",
       "...                                                 ...   \n",
       "6895  [0.14163919484467705, -0.1871535330863532, -0....   \n",
       "6896  [0.10089287837516506, -0.15370171061200275, -0...   \n",
       "6897  [0.30012685395650657, -0.2629061112392972, -0....   \n",
       "6898  [0.18598375894521132, -0.26601918530104635, -0...   \n",
       "6899  [0.14173209453104368, -0.1633085748664822, -0....   \n",
       "\n",
       "                                          bow_embedding  \\\n",
       "0     [4.111553694916485, -1.4091154149948346, -2.22...   \n",
       "1     [15.340171294779958, 0.6782201913070947, -3.95...   \n",
       "2     [17.579511089264834, -3.370470538008803, -6.03...   \n",
       "3     [13.623137823124184, 2.594215458815296, 1.1975...   \n",
       "4     [11.715673205868814, -3.8977639951627094, -0.5...   \n",
       "...                                                 ...   \n",
       "6895  [0.5081918596591085, -0.4770132598925509, 0.04...   \n",
       "6896  [0.34453471754679216, -0.49345817220335975, 0....   \n",
       "6897  [5.587042723074881, -3.018912737323734, -0.696...   \n",
       "6898  [1.8348212505816643, -2.5021137033474385, 0.45...   \n",
       "6899  [1.0371355773081052, -0.8878316745591224, -0.0...   \n",
       "\n",
       "                                          w2v_embedding  \\\n",
       "0     [0.023112578, 0.004341462, -0.028385904, 0.039...   \n",
       "1     [0.013536842, 0.009806181, 0.0037024287, 0.046...   \n",
       "2     [0.023063188, 0.059482925, 0.01734151, 0.06674...   \n",
       "3     [-0.03447485, 0.012470036, 0.040506534, 0.1266...   \n",
       "4     [-0.039161813, 0.050845865, 0.045167223, -0.00...   \n",
       "...                                                 ...   \n",
       "6895  [0.007478841, 0.028465271, 0.01225586, 0.05277...   \n",
       "6896  [0.03495687, 0.08366224, 0.03197157, 0.0962388...   \n",
       "6897  [0.0066373795, 0.04132683, 0.008552303, 0.0507...   \n",
       "6898  [0.032494266, 0.038722403, -0.01410776, 0.0764...   \n",
       "6899  [-0.034314055, 0.05650489, -0.031183043, 0.083...   \n",
       "\n",
       "                                      roberta_embedding  \n",
       "0     [-1.7613875, -0.29602507, 1.0549823, 0.4420643...  \n",
       "1     [0.5113651, -0.5632596, -0.08679409, 0.0578204...  \n",
       "2     [0.4061544, 0.033350937, -0.30923274, -0.04415...  \n",
       "3     [0.74233645, -0.8022974, 0.30516905, -0.150992...  \n",
       "4     [0.4902299, -0.04709744, -0.11096978, -0.33383...  \n",
       "...                                                 ...  \n",
       "6895  [-4.3445463, -0.82720375, -0.76975673, 0.55827...  \n",
       "6896  [-3.484783, -0.13741834, -0.2212331, 0.3501901...  \n",
       "6897  [0.060427, 0.93066996, -0.5383188, -0.11787002...  \n",
       "6898  [0.29304072, 1.2304765, -0.0635877, 0.6557827,...  \n",
       "6899  [-2.2836635, -0.034083482, 0.5534697, 1.026466...  \n",
       "\n",
       "[6900 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'tfidf_embedding', 'bow_embedding', 'w2v_embedding', 'roberta_embedding'],\n",
      "    num_rows: 6900\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "display(test_df)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a421c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:01<00:00,  4.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:17<00:00, 17.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/lelexuanzz/Gossipcop_Politifact_Test/commit/ae6f4376cc04e121873e5bae8beb733f740442d1', commit_message='Added roBERTa embeddings', commit_description='', oid='ae6f4376cc04e121873e5bae8beb733f740442d1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/lelexuanzz/Gossipcop_Politifact_Test', endpoint='https://huggingface.co', repo_type='dataset', repo_id='lelexuanzz/Gossipcop_Politifact_Test'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.push_to_hub(\"lelexuanzz/Gossipcop_Politifact_Test\", commit_message=\"Added roBERTa embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd01f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
