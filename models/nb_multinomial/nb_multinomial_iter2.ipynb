{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device for torch\n",
    "use_cuda = True\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "WELFake from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lelexuanzz/WELFake_stylo_feats\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset to pandas for easier implementation with sklearn\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 45\n",
    "\n",
    "y = df[\"label\"]\n",
    "x = df.drop(labels=[\"label\", \"text\"], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "display(x_train)\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB multinomial needs numeric, remove non numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this if future parts have ERRORS regarding numeric (unclean input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to pandas for easier implementation with sklearn\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "print(\"Dataset columns:\", df.columns)\n",
    "print(\"Data types:\", df.dtypes)\n",
    "\n",
    "# Check for any non-numeric values that might cause problems\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # Check if column contains strings\n",
    "        print(f\"Column {col} contains non-numeric data and will be dropped\")\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "# Make sure all remaining columns are numeric\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting column {col}: {e}\")\n",
    "        # If conversion fails, we'll examine the problematic values\n",
    "        problematic_rows = df[~pd.to_numeric(df[col], errors='coerce').notna()]\n",
    "        if not problematic_rows.empty:\n",
    "            print(f\"Sample of problematic values in {col}:\")\n",
    "            print(problematic_rows[col].head())\n",
    "            # Drop the problematic column if we can't convert it\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "print(\"Final columns after cleaning:\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "no further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Negative values to 0 and scale features to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and train multinomialNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_multinomial = MultinomialNB()\n",
    "nb_multinomial.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = nb_multinomial.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examine log probabilities for naive Bayes ( not a tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = nb_multinomial.feature_log_prob_[1] - nb_multinomial.feature_log_prob_[0]\n",
    "feature_names = x_train.columns\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Feature Importance for MultinomialNB')\n",
    "plt.barh(range(len(indices)), feature_importance[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Difference in Log Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV for MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Define parameter distribution for MultinomialNB\n",
    "param_dist = {'alpha': uniform(0.001, 10)}  # Randomly sample alpha values between 0.001 and 10\n",
    "\n",
    "# Use RandomizedSearchCV to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(MultinomialNB(), \n",
    "                                 param_distributions=param_dist, \n",
    "                                 n_iter=20, \n",
    "                                 cv=5, \n",
    "                                 n_jobs=-1, \n",
    "                                 random_state=seed)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(x_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha parameter with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters\n",
    "print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.4f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print randomized search results\n",
    "print(\"\\nBest parameters found by RandomizedSearchCV: \", rand_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.4f}\".format(rand_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nb = grid_search.best_estimator_\n",
    "y_pred_best = best_nb.predict(x_test_scaled)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(\"Test accuracy with best parameters: {:.4f}\".format(accuracy_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with the best model\n",
    "y_pred = best_nb.predict(x_test)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a dictionary to store all results\n",
    "results = {\n",
    "    \"model_name\": \"MultinomialNB_iter2\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset\": \"WELFake_stylo_feats\",\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"best_accuracy\": float(accuracy_best),\n",
    "        \"best_alpha\": grid_search.best_params_[\"alpha\"],\n",
    "        \"confusion_matrix\": conf_matrix.tolist(),\n",
    "        \"classification_report\": {}\n",
    "    },\n",
    "    \"feature_importance\": {feature: float(importance) for feature, importance in \n",
    "                          zip(feature_names, feature_importance)}\n",
    "}\n",
    "\n",
    "# Get classification report metrics\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "for class_label, metrics in report.items():\n",
    "    if isinstance(metrics, dict):\n",
    "        results[\"metrics\"][\"classification_report\"][class_label] = {\n",
    "            k: float(v) for k, v in metrics.items()\n",
    "        }\n",
    "\n",
    "# Ensure the results directory exists\n",
    "if not os.path.exists('model_results'):\n",
    "    os.makedirs('model_results')\n",
    "\n",
    "# Save the results to a JSON file\n",
    "filename = f\"model_results/nb_multinomial_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Setup k-fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a new MultinomialNB classifier with the best alpha from grid search\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "nb_cv = MultinomialNB(alpha=best_alpha)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(nb_cv, x_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"K-fold Cross-Validation Results ({k_folds} folds):\")\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "print(f\"Individual fold scores: {cv_scores}\")\n",
    "\n",
    "# Train the final model on the entire training dataset\n",
    "nb_final = MultinomialNB(alpha=best_alpha)\n",
    "nb_final.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the gossicop dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the external test dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_dataset = load_dataset(\"lelexuanzz/Gossipcop_Politifact_Test_Stylo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_dataset[\"train\"].to_pandas()\n",
    "display(test_df)\n",
    "\n",
    "\n",
    "y_test_set = test_df[\"label\"]\n",
    "x_test_set = test_df.drop(labels=[\"label\", \"text\"], axis=1)\n",
    "\n",
    "display(x_test_set)\n",
    "display(y_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_nb.predict(x_test_set)\n",
    "accuracy = accuracy_score(y_test_set, y_test_pred)\n",
    "precision = precision_score(y_test_set, y_test_pred)\n",
    "recall = recall_score(y_test_set, y_test_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Remove the text column if present in your x_train dataset\n",
    "features_for_selection = x_train.drop('text', axis=1, errors='ignore')\n",
    "\n",
    "# Define the number of top features to select\n",
    "num_features = 5  # You can adjust this number\n",
    "\n",
    "# Use SelectKBest to identify top features\n",
    "selector = SelectKBest(f_classif, k=num_features)\n",
    "selector.fit(features_for_selection, y_train)\n",
    "\n",
    "# Get selected feature indices and names\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features = features_for_selection.columns[selected_indices]\n",
    "\n",
    "print(f\"\\nTop {num_features} selected features:\")\n",
    "for i, feature in enumerate(selected_features):\n",
    "    print(f\"{i+1}. {feature} (Score: {selector.scores_[selected_indices[i]]:.2f})\")\n",
    "\n",
    "# Create datasets with only selected features\n",
    "X_train_selected = features_for_selection.iloc[:, selected_indices]\n",
    "X_test_selected = x_test.drop('text', axis=1, errors='ignore').iloc[:, selected_indices]\n",
    "\n",
    "# Scale the selected features\n",
    "X_train_selected_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_selected_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Train and evaluate model with selected features\n",
    "nb_selected = MultinomialNB(alpha=best_alpha)\n",
    "nb_selected.fit(X_train_selected_scaled, y_train)\n",
    "selected_predictions = nb_selected.predict(X_test_selected_scaled)\n",
    "\n",
    "# Evaluate performance with selected features\n",
    "selected_accuracy = accuracy_score(y_test, selected_predictions)\n",
    "selected_report = classification_report(y_test, selected_predictions)\n",
    "\n",
    "print(\"\\nPerformance with selected features:\")\n",
    "print(f\"Accuracy: {selected_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(selected_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "\n",
    "def recursive_feature_pruning(model, x, y, num_features_to_select=None):\n",
    "\n",
    "    rfe = RFE(estimator=model, n_features_to_select=num_features_to_select or 1, step=1)\n",
    "    rfe.fit(x, y)\n",
    "\n",
    "    # Get selected features\n",
    "    selected_features = x.columns[rfe.support_]\n",
    "    \n",
    "    # Get ranking of features\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        'feature': x.columns,\n",
    "        'rank': rfe.ranking_\n",
    "    }).sort_values(by='rank')\n",
    "\n",
    "    return selected_features.tolist(), feature_ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Use LogisticRegression as the estimator for RFE\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "\n",
    "# Perform recursive feature elimination\n",
    "top_features, ranking_df = recursive_feature_pruning(logistic_model, x_train, y_train, num_features_to_select=10)\n",
    "\n",
    "print(\"Selected Top Features:\")\n",
    "print(top_features)\n",
    "\n",
    "print(\"\\nFeature Rankings:\")\n",
    "print(ranking_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with top features after Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(x_train[top_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train[top_features], y_train)\n",
    "\n",
    "best_nb_multinomial_rfe = rand_search.best_estimator_\n",
    "print('Best hyperparameters:',  rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfe = best_nb_multinomial_rfe.predict(x_test[top_features])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_rfe)\n",
    "precision = precision_score(y_test, y_pred_rfe)\n",
    "recall = recall_score(y_test, y_pred_rfe)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
